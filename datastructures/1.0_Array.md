# Array

## Quick Definition

A contiguous block of memory storing elements of the same type, accessible in constant time by index. Foundation for most other data structures.

## Big-O Summary

| Operation | Time | Space |
|-----------|------|-------|
| Access    | **O(1)** | O(n) |
| Search    | O(n) | — |
| Insert    | O(n)* | — |
| Delete    | O(n)* | — |
*Amortized O(1) for append in dynamic arrays

## Core Operations

```java
// Multiple initialization methods
ArrayList<Integer> nums1 = new ArrayList<>();
ArrayList<Integer> nums2 = new ArrayList<>(Arrays.asList(3, 9, 2));
ArrayList<Integer> nums3 = new ArrayList<>(List.of(1, 4, 7));

// Access and update
int x = nums2.get(1);           // get: 9
nums2.set(2, 5);               // update: [3, 9, 5]

// Insert operations
nums1.add(10);                 // append: O(1) amortized
nums2.add(1, 99);              // insert at index: O(n)

// Delete operations  
nums2.remove(0);               // remove by index: O(n)
nums2.remove(Integer.valueOf(99)); // remove by value: O(n)

// Iteration methods
for (int val : nums2) System.out.print(val + " ");
nums2.forEach(System.out::println);
```

## Python Snippet

```python
# Multiple initialization methods
nums1 = []
nums2 = [3, 9, 2]
nums3 = list((1, 4, 7))

# Access and update
x = nums2[1]           # 9
nums2[2] = 5           # [3, 9, 5]

# Insert operations
nums1.append(10)       # amortized O(1)
nums2.insert(1, 99)    # O(n)

# Delete operations
nums2.pop(0)           # remove by index
nums2.remove(99)       # remove by value

# Iteration methods
for val in nums2:
    print(val)
```

## When to Use

- Random access patterns dominate (lookups, updates by index)
- Memory-efficient storage for primitive types or objects
- Base for implementing stacks, queues, heaps, hash tables
- Streaming data where append operations are frequent
- Mathematical computations requiring vectorized operations

## Trade-offs

**Pros:**

- O(1) indexed access and updates
- Cache-friendly due to memory locality  
- Low memory overhead per element
- Excellent for sequential access patterns

**Cons:**

- O(n) insertion/deletion in middle positions
- Fixed capacity (static arrays) or resize overhead (dynamic)
- Memory waste during capacity growth phases

## Practice Problems

- **Two Sum**: Find indices of two numbers that add to target
- **Rotate Array**: Rotate array k steps right in-place  
- **Maximum Subarray (Kadane's)**: Find contiguous subarray with largest sum
- **Move Zeroes**: Move all 0s to end while maintaining relative order
- **Merge Intervals**: Merge overlapping intervals after sorting

<details>
<summary>Implementation Notes (Advanced)</summary>

### Memory Layout

Elements stored contiguously in memory → excellent cache locality. Each access loads adjacent elements into cache line, benefiting sequential operations.

### Dynamic Array Growth

- **Growth factor**: Typically 1.5x or 2x when capacity exceeded
- **Amortized analysis**: Despite O(n) resize cost, append averages O(1)
- **Memory overhead**: 25-50% unused capacity in worst case

### Performance Considerations

- **Cache misses**: Random access patterns can thrash cache
- **Branch prediction**: Sorted arrays enable efficient binary search
- **SIMD opportunities**: Vectorized operations on primitive arrays

</details>
