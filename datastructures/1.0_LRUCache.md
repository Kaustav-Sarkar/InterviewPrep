# LRU Cache

## Quick Definition

Cache with Least Recently Used eviction policy. Removes least recently accessed items when capacity is exceeded. Provides O(1) get/put operations.

## Big-O Summary

| Operation | Time | Space |
|-----------|------|-------|
| Get       | **O(1)** | O(capacity) |
| Put       | **O(1)** | — |
| Contains  | **O(1)** | — |
| Remove    | **O(1)** | — |

## Core Operations

```java
// Using LinkedHashMap (built-in LRU)
class LRUCacheSimple extends LinkedHashMap<Integer, Integer> {
    private int capacity;
    
    public LRUCacheSimple(int capacity) {
        super(capacity, 0.75f, true);  // access-order = true for LRU
        this.capacity = capacity;
    }
    
    @Override
    protected boolean removeEldestEntry(Map.Entry<Integer, Integer> eldest) {
        return size() > capacity;  // auto-evict when exceeding capacity
    }
}

// Custom implementation with HashMap + Doubly Linked List
class LRUCache {
    class Node {
        int key, value;
        Node prev, next;
        Node(int key, int value) { this.key = key; this.value = value; }
    }
    
    private int capacity;
    private Map<Integer, Node> cache = new HashMap<>();
    private Node head = new Node(0, 0);  // dummy head
    private Node tail = new Node(0, 0);  // dummy tail
    
    public LRUCache(int capacity) {
        this.capacity = capacity;
        head.next = tail;
        tail.prev = head;
    }
    
    public int get(int key) {
        Node node = cache.get(key);
        if (node == null) return -1;
        
        moveToHead(node);  // mark as recently used
        return node.value;
    }
    
    public void put(int key, int value) {
        Node existing = cache.get(key);
        if (existing != null) {
            existing.value = value;
            moveToHead(existing);
            return;
        }
        
        Node newNode = new Node(key, value);
        cache.put(key, newNode);
        addToHead(newNode);
        
        if (cache.size() > capacity) {
            Node tail = removeTail();
            cache.remove(tail.key);
        }
    }
    
    private void addToHead(Node node) {
        node.prev = head;
        node.next = head.next;
        head.next.prev = node;
        head.next = node;
    }
    
    private void removeNode(Node node) {
        node.prev.next = node.next;
        node.next.prev = node.prev;
    }
    
    private void moveToHead(Node node) {
        removeNode(node);
        addToHead(node);
    }
    
    private Node removeTail() {
        Node last = tail.prev;
        removeNode(last);
        return last;
    }
}

// Usage examples
LRUCache cache = new LRUCache(2);
cache.put(1, 10);    // cache: {1=10}
cache.put(2, 20);    // cache: {1=10, 2=20}
cache.get(1);        // returns 10, cache: {2=20, 1=10} (1 moved to front)
cache.put(3, 30);    // evicts key 2, cache: {1=10, 3=30}

// Thread-safe version using ConcurrentHashMap
class ThreadSafeLRUCache {
    private final ConcurrentHashMap<Integer, Integer> cache;
    
    public ThreadSafeLRUCache(int capacity) {
        this.cache = new ConcurrentHashMap<Integer, Integer>(capacity, 0.75f, true) {
            @Override
            protected boolean removeEldestEntry(Map.Entry<Integer, Integer> eldest) {
                return size() > capacity;
            }
        };
    }
}
```

## Python Snippet

```python
from collections import OrderedDict

# Using OrderedDict (access-order)
class LRUCacheSimple(OrderedDict):
    def __init__(self, capacity: int):
        super().__init__()
        self.capacity = capacity
    def get(self, key: int) -> int:
        if key not in self: return -1
        self.move_to_end(key)  # mark as recently used
        return self[key]
    def put(self, key: int, value: int) -> None:
        if key in self:
            self.move_to_end(key)
        self[key] = value
        if len(self) > self.capacity:
            self.popitem(last=False)  # evict LRU

# Custom dict + doubly linked list
class Node:
    __slots__ = ("k", "v", "prev", "next")
    def __init__(self, k, v):
        self.k, self.v, self.prev, self.next = k, v, None, None

class LRUCache:
    def __init__(self, capacity: int):
        self.cap = capacity
        self.map = {}
        self.head, self.tail = Node(0, 0), Node(0, 0)
        self.head.next = self.tail; self.tail.prev = self.head
    def _add(self, node):
        node.prev = self.head; node.next = self.head.next
        self.head.next.prev = node; self.head.next = node
    def _remove(self, node):
        p, n = node.prev, node.next
        p.next = n; n.prev = p
    def _move_to_head(self, node):
        self._remove(node); self._add(node)
    def _pop_tail(self):
        last = self.tail.prev; self._remove(last); return last
    def get(self, key: int) -> int:
        node = self.map.get(key)
        if not node: return -1
        self._move_to_head(node); return node.v
    def put(self, key: int, value: int) -> None:
        node = self.map.get(key)
        if node:
            node.v = value; self._move_to_head(node)
        else:
            node = Node(key, value); self.map[key] = node; self._add(node)
            if len(self.map) > self.cap:
                last = self._pop_tail(); del self.map[last.k]
```

## When to Use

- CPU caches and memory hierarchies
- Web browser caches for pages and resources
- Database buffer pools and page replacement
- CDN edge caches for content delivery
- API response caching with capacity limits

## Trade-offs

**Pros:**

- O(1) access and update operations
- Simple eviction policy that works well in practice
- Good temporal locality exploitation
- Easy to implement with existing data structures

**Cons:**

- Fixed capacity requires careful sizing
- No consideration of item size or cost
- Poor performance for sequential access patterns
- Can evict frequently accessed items during bursts

## Practice Problems

- **LRU Cache**: Implement get() and put() in O(1) time
- **LFU Cache**: Implement Least Frequently Used variant
- **Design Browser History**: Back/forward with capacity limits
- **Page Replacement**: Simulate OS virtual memory management
- **All O(1) Data Structure**: Insert, delete, getRandom in O(1)

<details>
<summary>Implementation Notes (Advanced)</summary>

### LinkedHashMap Implementation

- **Built-in support**: Java's LinkedHashMap has access-order mode
- **Simplicity**: Override removeEldestEntry for automatic eviction
- **Thread safety**: Not thread-safe by default, requires external synchronization

### Custom Implementation Details

- **HashMap + Doubly Linked List**: Combines O(1) lookup with O(1) reordering
- **Dummy nodes**: Simplify boundary conditions in linked list operations
- **Move-to-front**: Recently accessed items move to head of list

### Memory Considerations

- **Node overhead**: Each cache entry requires ~32-40 bytes (key, value, pointers)
- **HashMap load factor**: Default 0.75 provides good time/space tradeoff
- **Memory fragmentation**: Frequent allocation/deallocation can fragment heap

### Performance Optimizations

- **Batch updates**: Group multiple operations to reduce overhead
- **Size-aware eviction**: Consider item size, not just count
- **Probabilistic eviction**: Random sampling for better performance at scale

### Alternative Policies

- **LFU (Least Frequently Used)**: Tracks access frequency instead of recency
- **ARC (Adaptive Replacement Cache)**: Combines LRU and LFU adaptively
- **Clock algorithm**: Approximates LRU with single bit per page

</details>
